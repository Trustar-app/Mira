# app.py
from datetime import datetime
from graphs.mira_graph import mira_graph
from tools.common.formatters import format_messages, dict_to_markdown
from tools.common.utils import video_to_text, fill_config_with_env
import gradio as gr
from langgraph.types import Command
from utils.loggers import MiraLog
import uuid
from state import default_app_state
from frontend.config_tab import render_config_tab
from frontend.profile_tab import render_profile_tab
from frontend.products_tab import render_products_tab
from frontend.custom_css import custom_css
from dotenv import load_dotenv
from config import MIRA_GREETING_PROMPT

load_dotenv()

def get_current_time_and_season():
    now = datetime.now()
    
    season_map = {
        (12, 1, 2): "å†¬å­£",
        (3, 4, 5): "æ˜¥å­£",
        (6, 7, 8): "å¤å­£",
        (9, 10, 11): "ç§‹å­£"
    }
    
    current_season = next(season for months, season in season_map.items() if now.month in months)
    current_time = now.strftime("%H:%M")
    
    return current_time, current_season

def generate_greeting_prompt(app_state):
    current_time, season = get_current_time_and_season()
    user_profile = app_state['profile']
    products_directory = app_state['products']
    greeting_prompt = MIRA_GREETING_PROMPT.format(
        current_time=current_time,
        season=season,
        user_profile=user_profile,
        products_directory=products_directory
    )
    return greeting_prompt

def combine_msg(chat, msg):
    # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯è¿›åº¦ä¿¡æ¯ï¼Œåˆ™å¯ä»¥è¦†ç›–
    if chat and chat[-1].get("type") == "progress":
        chat[-1]["content"] = msg["content"]
        chat[-1]["type"] = msg["type"]
    else:
        chat.append({"role": "assistant", "content": msg["content"], "type": msg["type"]})
    return chat

def extract_profile_values(profile):
    return [
        profile.get('name', ''),
        profile.get('gender', ''),
        profile.get('age', ''),
        profile.get('face_features', {}).get('face_shape', ''),
        profile.get('face_features', {}).get('eyes', ''),
        profile.get('face_features', {}).get('nose', ''),
        profile.get('face_features', {}).get('mouth', ''),
        profile.get('face_features', {}).get('eyebrows', ''),
        profile.get('skin_color', ''),
        profile.get('skin_type', ''),
        profile.get('skin_quality', {}).get('spot', 0) or 0,
        profile.get('skin_quality', {}).get('wrinkle', 0) or 0,
        profile.get('skin_quality', {}).get('pore', 0) or 0,
        profile.get('skin_quality', {}).get('redness', 0) or 0,
        profile.get('skin_quality', {}).get('oiliness', 0) or 0,
        profile.get('skin_quality', {}).get('acne', 0) or 0,
        profile.get('skin_quality', {}).get('dark_circle', 0) or 0,
        profile.get('skin_quality', {}).get('eye_bag', 0) or 0,
        profile.get('skin_quality', {}).get('tear_trough', 0) or 0,
        profile.get('skin_quality', {}).get('firmness', 0) or 0,
        profile.get('makeup_skill_level', 0) or 0,
        profile.get('skincare_skill_level', 0) or 0,
        profile.get('user_preferences', ''),
    ]

def extract_products_values(products):
    from frontend.products_tab import render_products_collection
    choices = [(p['name'], i) for i, p in enumerate(products)]
    return [
        render_products_collection(products),
        gr.update(choices=choices, value=None)
    ]

def extract_config_values(config):
    return [
        config.get('chat_api_key', ''),
        config.get('chat_api_base', ''),
        config.get('chat_model_name', ''),
        config.get('voice_model_name', ''),
        config.get('character_setting', {}).get('name', ''),
        config.get('character_setting', {}).get('personality', ''),
        config.get('character_setting', {}).get('background', ''),
        config.get('character_setting', {}).get('tone', ''),
        config.get('character_setting', {}).get('expertise', ''),
        config.get('character_setting', {}).get('interaction_style', ''),
        config.get('tavily_api_key', ''),
        config.get('use_youcam', False),
        config.get('youcam_api_key', ''),
        config.get('youcam_secret_key', '')
    ]

def process_user_input(video, text, chat, markdown, state):
    if text:
        chat.append({"role": "user", "content": text, "type": "final"})
    if video:
        chat.append({"role": "user", "content": gr.Video(video), "type": "final"})
    yield chat, "", state, None, "", *extract_profile_values(state['profile']), *extract_products_values(state['products']), *extract_config_values(state['config'])

    if video:
        progress_message = "æ­£åœ¨å¤„ç†è§†é¢‘è¾“å…¥..."
        chat = combine_msg(chat, {"content": progress_message, "type": "progress"})
        yield chat, "", state, None, "", *extract_profile_values(state['profile']), *extract_products_values(state['products']), *extract_config_values(state['config'])
        text += "\n<è§†é¢‘ä¸­è¯´è¯å†…å®¹>\n" + video_to_text(video) + "\n</è§†é¢‘ä¸­è¯´è¯å†…å®¹>"
    if state.get('resume'):
        inputs = Command(
            resume={
                "text": text,
                "video": video,
            }
        )
        state['resume'] = False
    else:
        inputs = {
            "messages": format_messages(video, text),
            "user_profile": state['profile'],
            "products_directory": state['products'],
        }
    
    config_for_graph = fill_config_with_env(state['config'])
    for mode, step in mira_graph.stream(inputs, {"configurable": config_for_graph}, stream_mode=["custom", "updates"]):
        if mode == "updates" and not "__interrupt__" in step:
            continue
        if "__interrupt__" in step:
            content = step.get("__interrupt__")[0].value.get("content")
            chat = combine_msg(chat, {"content": content, "type": "final"})
            state['resume'] = True
            yield chat, markdown, state, None, "", *extract_profile_values(state['profile']), *extract_products_values(state['products']), *extract_config_values(state['config'])
            break
        msg_type = step.get("type")
        content = step.get("content")
        MiraLog("app", f"msg_type: {msg_type}")
        if msg_type == "progress":
            chat = combine_msg(chat, {"content": content, "type": "progress"})
            yield chat, markdown, state, None, "", *extract_profile_values(state['profile']), *extract_products_values(state['products']), *extract_config_values(state['config'])

        elif msg_type == "final":
            markdown = dict_to_markdown(content['markdown']) if content.get('markdown') else markdown
            state['profile'].update(content['profile']) if content.get('profile') else None
            state['products'].append(content['product']) if content.get('product') else None
            chat = combine_msg(chat, {"content": content["response"], "type": "final"}) if content.get("response") else chat
            yield chat, markdown, state, None, "", *extract_profile_values(state['profile']), *extract_products_values(state['products']), *extract_config_values(state['config'])

        else:
            yield chat, markdown, state, None, "", *extract_profile_values(state['profile']), *extract_products_values(state['products']), *extract_config_values(state['config'])


def new_chat(state):
            state['config']['thread_id'] = str(uuid.uuid4())
            state['resume'] = False
            greeting_prompt = generate_greeting_prompt(state)
            state['config']['greeting_prompt'] = greeting_prompt
            greeting_response = mira_graph.invoke({"messages": format_messages(None, greeting_prompt)}, {"configurable": fill_config_with_env(state['config'])}, stream_mode=["custom"])
            chat = []
            response = ""
            first_chunk = True  
            for mode, chunk in greeting_response:
                if mode == "custom" and chunk['type'] == "progress":
                    if first_chunk:
                        first_chunk = False
                        continue
                    else:
                        response = chunk['content']
                    yield combine_msg(chat, {"content": response, "type": "progress"}), "", state, None, "", *extract_profile_values(state['profile']), *extract_products_values(state['products']), *extract_config_values(state['config'])
                elif mode == "custom" and chunk['type'] == "final":
                    response = chunk['content']['response']
                    yield combine_msg(chat, {"content": response, "type": "final"}), "", state, None, "", *extract_profile_values(state['profile']), *extract_products_values(state['products']), *extract_config_values(state['config'])

def build_demo():
    with gr.Blocks(theme=gr.themes.Soft(), css=custom_css) as demo:
        gr.Markdown("<div class='title'>ğŸ€ Mira æ™ºèƒ½åŒ–å¦†é•œ</div><div class='subtitle'>AIèµ‹èƒ½ä½ çš„ç¾ä¸½æ—¥å¸¸</div>", elem_id="main-title")
        with gr.Accordion("ğŸ‘‹ æ¬¢è¿æ¥åˆ° Miraï¼æˆ‘æ˜¯ä¸€é¢æ™ºèƒ½é•œå­ï¼Œä¹Ÿæ˜¯ä½ çš„ç§äººç¾å¦†åŠ©ç†å’Œç¾ä¸½é¡¾é—®ã€‚", open=False):
            gr.Markdown("""
            æˆ‘å¯ä»¥ï¼š
            * ğŸ” **äº§å“åˆ†æ** - "è¿™ä¸ªæŠ¤è‚¤å“æ€ä¹ˆæ ·ï¼Ÿ" æˆ– "æ¨èé€‚åˆæˆ‘çš„ç²‰åº•æ¶²"
            * ğŸ’„ **è‚¤è´¨æ£€æµ‹** - "å¸®æˆ‘æ£€æµ‹ä¸‹çš®è‚¤çŠ¶å†µ" æˆ– "æˆ‘çš„çš®è‚¤æœ‰ä»€ä¹ˆé—®é¢˜"
            * ğŸ‘©â€ğŸ« **ç¾å¦†æŒ‡å¯¼** - "æ•™æˆ‘ç”»ä¸€ä¸ªçº¦ä¼šå¦†" æˆ– "æ•™æˆ‘æŠ¤è‚¤"
            * ğŸ“ **ä¸ªäººæ¡£æ¡ˆ** - "æˆ‘æƒ³åˆ›å»ºæˆ‘çš„æ¡£æ¡ˆ" æˆ– "æ›´æ–°æˆ‘çš„æ¡£æ¡ˆ"
            
            è¿™ä¸ªdemoæ—¨åœ¨æ¨¡æ‹Ÿä½ ä¸æ™ºèƒ½é•œå­çš„äº’åŠ¨ä½“éªŒ~
            """, elem_classes="compact-markdown")
        app_state = gr.State(default_app_state())
        with gr.Tab("ğŸ’¬ èŠå¤©"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("#### ğŸ“¥ ç”¨æˆ·è¾“å…¥åŒº")
                    with gr.Accordion("ğŸ’¡ ä¸ Mira å¯¹è¯å°±åƒå’Œæœ‹å‹è§†é¢‘èŠå¤©ä¸€æ ·ï¼š", open=False):
                        gr.Markdown("""
                        * å¯ä»¥é€šè¿‡è§†é¢‘å½•åˆ¶æ¥å±•ç¤ºå’Œè¯¢é—®
                        * ä¹Ÿå¯ä»¥ç›´æ¥è¾“å…¥æ–‡å­—äº¤æµ
                        """, elem_classes="compact-markdown")
                    video_in = gr.Video(sources=["webcam"], include_audio=True, label="è§†é¢‘å¯¹è¯ï¼ˆå«éŸ³é¢‘ï¼‰")
                    text_in = gr.Textbox(label="æ–‡å­—å¯¹è¯", lines=2, placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜æˆ–éœ€æ±‚â€¦")
                    submit_btn = gr.Button("æäº¤", elem_id="submit-btn")
                    new_chat_btn = gr.Button("æ–°å»ºå¯¹è¯", elem_id="new-chat-btn")
                with gr.Column(scale=2):
                    gr.Markdown("#### ğŸ¤– AIå¯¹è¯åŒº")
                    with gr.Accordion("ğŸ’¡ è¿™é‡Œä¼šæ˜¾ç¤º Mira çš„å›åº”ï¼Œå°±åƒä½ åœ¨é•œå­å‰å¬åˆ°çš„å¯¹è¯ä¸€æ ·ï¼š", open=False):
                        gr.Markdown("""
                        Mira ä¼šï¼š
                        * ğŸ¯ ç†è§£ä½ çš„éœ€æ±‚
                        * ğŸ’ ç»™å‡ºä¸ªæ€§åŒ–å»ºè®®
                        * ğŸ“ è®°å½•é‡è¦ä¿¡æ¯
                        """, elem_classes="compact-markdown")
                    greeting_prompt = generate_greeting_prompt(app_state.value)
                    app_state.value['config']['greeting_prompt'] = greeting_prompt
                    greeting_response = mira_graph.invoke({"messages": format_messages(None, greeting_prompt)}, {"configurable": fill_config_with_env(app_state.value['config'])}, stream_mode=["custom"])
                    response = ""
                    for mode, chunk in greeting_response:
                        if mode == "custom" and chunk['type'] == "final":
                            response = chunk['content']['response']
                    chat_out = gr.Chatbot(label="AIå¯¹è¯", value=[{"role": "assistant", "content": response, "type": "final"}], elem_id="chat-out", type="messages")
                with gr.Column(scale=1):
                    gr.Markdown("#### ğŸ” åˆ†æç»“æœ")
                    with gr.Accordion("ğŸ’¡ è¿™é‡Œä¼šæ˜¾ç¤ºæ›´è¯¦ç»†çš„åˆ†æç»“æœï¼š", open=False):
                        gr.Markdown("""
                        * ğŸ”¬ çš®è‚¤æ£€æµ‹ç»“æœ
                        * ğŸ” äº§å“åˆ†æç»“æœ
                        * ğŸ’„ åŒ–å¦†æˆ–æŠ¤è‚¤æŒ‡å¯¼è®¡åˆ’
                        """, elem_classes="compact-markdown")
                    markdown_out = gr.Markdown(label="åˆ†æç»“æœ", elem_id="feedback-md")
                    clear_btn = gr.Button("æ¸…ç©º", elem_id="clear-btn")
        with gr.Tab("ğŸ‘¤ ä¸ªäººæ¡£æ¡ˆ"):
            profile_widgets = render_profile_tab(app_state)
        with gr.Tab("ğŸ’„ äº§å“å¡ç‰‡é›†"):
            products_widgets = render_products_tab(app_state)
        with gr.Tab("ğŸ› é…ç½®"):
            config_widgets = render_config_tab(app_state)


        submit_btn.click(
            process_user_input,
            inputs=[video_in, text_in, chat_out, markdown_out, app_state],
            outputs=[chat_out, markdown_out, app_state, video_in, text_in] + profile_widgets + products_widgets + config_widgets
        )

        new_chat_btn.click(
            new_chat,
            inputs=[app_state],
            outputs=[chat_out, markdown_out, app_state, video_in, text_in] + profile_widgets + products_widgets + config_widgets
        )

        clear_btn.click(
            lambda markdown: "",
            inputs=[markdown_out],
            outputs=[markdown_out]
        )
    return demo

demo = build_demo()
demo.queue()
demo.launch(show_error=True, max_threads=10)